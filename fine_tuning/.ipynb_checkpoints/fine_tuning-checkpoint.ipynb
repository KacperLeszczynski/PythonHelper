{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09d61965-63b5-4e6c-ac62-10e89c9406f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import chromadb\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e39a261-874f-4511-b6fc-e1db0e72f853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a7e59b-e626-4e24-9fce-1a9c14294392",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfdeb3f7-f565-472c-96fd-4ddfc43c5949",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPEN_AI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54adecb9-bcad-427d-ab8b-12af698c758d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mbpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a99630b5-b2c8-4c3c-8dec-1264487238f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 374\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    prompt: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0f2fc5d-24b1-49cd-9c68-f2b1632b2d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class Pair(object): \n",
      "\tdef __init__(self, a, b): \n",
      "\t\tself.a = a \n",
      "\t\tself.b = b \n",
      "def max_chain_length(arr, n): \n",
      "\tmax = 0\n",
      "\tmcl = [1 for i in range(n)] \n",
      "\tfor i in range(1, n): \n",
      "\t\tfor j in range(0, i): \n",
      "\t\t\tif (arr[i].a > arr[j].b and\n",
      "\t\t\t\tmcl[i] < mcl[j] + 1): \n",
      "\t\t\t\tmcl[i] = mcl[j] + 1\n",
      "\tfor i in range(n): \n",
      "\t\tif (max < mcl[i]): \n",
      "\t\t\tmax = mcl[i] \n",
      "\treturn max\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0]['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12f16954-d3a1-4495-9c53-85a0c1a52f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_id': 601, 'text': 'Write a function to find the longest chain which can be formed from the given set of pairs.', 'code': 'class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max', 'test_list': ['assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3', 'assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4', 'assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5'], 'test_setup_code': '', 'challenge_test_list': []}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d74ca52b-0ed8-485e-bacd-2a5512c6dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_model = \"\"\"\n",
    "    ### Context:\n",
    "    {context}\n",
    "    \n",
    "    ### User's questions:\n",
    "    {query}\n",
    "\n",
    "    ### Python version: \n",
    "    {python_version}\n",
    "    \n",
    "    ** Instructions **\n",
    "    - If user asks you to generate code and by using context you cannot do it, then generate it on your own\n",
    "    - If user doesn't ask to generate code and the context does not contain answer for query say \"I don't have sufficient knowledge to answer this question\".\n",
    "    - If the user's question does not specify Python, rephrase it internally as a Python-related question before answering.\n",
    "    - If there is a code in your output explain this code to the user step by step\n",
    "    - Do not answer any other question than about python programming language\n",
    "    - If topic is complex provide summary at the end of your answer\n",
    "    - Do not make up any information\n",
    "    - Provide consise and structured answer\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c026770a-82f3-4c3b-be5d-152210a7d53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_versions=[\"3.10\",\"3.11\",\"3.12\",\"3.13\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0898859a-e2bc-4f88-8060-1b824edaddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DB_PATH = \"./../data_collecting/chroma_db\"\n",
    "client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = client.get_or_create_collection(name=\"python_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91413e21-e1f8-4ade-93a2-dd022981cb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(text):\n",
    "    response = openai.embeddings.create(\n",
    "        input=[text],\n",
    "        model=\"text-embedding-ada-002\"\n",
    "    )\n",
    "    return response.data[0].embedding\n",
    "\n",
    "def retrieve_documents(query, python_version, top_k=7):\n",
    "    query_embedding = get_openai_embedding(query)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k,\n",
    "        where={\"version\": python_version}\n",
    "    )\n",
    "\n",
    "    return results[\"documents\"][0] if \"documents\" in results and results[\"documents\"] else []\n",
    "\n",
    "def format_query(user_query: str):\n",
    "    if \"python\" not in user_query.lower():\n",
    "        return f\"{user_query} in Python\"\n",
    "    return user_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fd0c0ee9-74cd-4c42-8ed1-4e6789b0e350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "699da972-95ea-49db-874f-1f05a4474e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "120\n",
      "140\n",
      "160\n",
      "180\n",
      "200\n",
      "220\n",
      "240\n",
      "260\n",
      "280\n",
      "300\n",
      "320\n",
      "340\n",
      "360\n"
     ]
    }
   ],
   "source": [
    "retrieved_context_train = []\n",
    "\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "    query = format_query(example[\"text\"])\n",
    "    retrieved_docs = retrieve_documents(query, python_version, 2)\n",
    "    retrieved_docs = \"\\n\\n\".join(retrieved_docs)\n",
    "    retrieved_context_train.append(retrieved_docs)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d30c0ec-d445-4d6b-bea1-cec689a8ecb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0795ead-0a5c-43df-b76d-e639175ccf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "retrieved_context_val = []\n",
    "\n",
    "for i, example in enumerate(dataset[\"validation\"]):\n",
    "    query = format_query(example[\"text\"])\n",
    "    retrieved_docs = retrieve_documents(query, python_version, 2)\n",
    "    retrieved_docs = \"\\n\\n\".join(retrieved_docs)\n",
    "    retrieved_context_val.append(retrieved_docs)\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0d0596eb-736d-41c4-9e52-39dc71a3a823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10\n",
      "3.11\n",
      "3.12\n",
      "3.13\n"
     ]
    }
   ],
   "source": [
    "formatted_data = []\n",
    "\n",
    "\n",
    "for python_version in python_versions:\n",
    "    for example, context in zip(dataset[\"train\"],retrieved_context_train):\n",
    "        formatted_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt_model.format(context=context, query=example[\"text\"], python_version=python_version).strip()},\n",
    "                {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"code\"]}\n",
    "            ]\n",
    "        })\n",
    "    print(python_version)\n",
    "\n",
    "# with open(\"mbpp_finetune.jsonl\", \"w\") as f:\n",
    "#     for entry in formatted_data:\n",
    "#         f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f61f6-26e7-49d5-97b8-6597df50372f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6c2271ff-e457-44a6-a82b-c96f57f0d8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = []\n",
    "for python_version in python_versions:\n",
    "    for example, context in zip(dataset[\"validation\"],retrieved_context_val):\n",
    "        validation_data.append({\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": prompt_model.format(context=context, query=example[\"text\"], python_version=python_version).strip()},\n",
    "                {\"role\": \"user\", \"content\": example[\"text\"]},\n",
    "                {\"role\": \"assistant\", \"content\": example[\"code\"]}\n",
    "            ]\n",
    "        })\n",
    "\n",
    "# with open(\"mbpp_finetune_val.jsonl\", \"w\") as f:\n",
    "#     for entry in validation_data:\n",
    "#         f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9a668956-4f54-4943-961e-9bebef575eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"mbpp_finetune.jsonl\", \"w\") as f:\n",
    "    for entry in formatted_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "with open(\"mbpp_finetune_val.jsonl\", \"w\") as f:\n",
    "    for entry in validation_data:\n",
    "        f.write(json.dumps(entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a5fa89bb-71ac-4d6f-af7f-53c9627bb304",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = openai.files.create(\n",
    "  file=open(\"mbpp_finetune.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "valid_file = openai.files.create(\n",
    "  file=open(\"mbpp_finetune_val.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fabffe96-cb58-4cd2-9e5f-f5b4d9ee6db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file Info: FileObject(id='file-TLoZTPyqSHzprWM2vPEdLH', bytes=2054116, created_at=1741724696, filename='mbpp_finetune.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n",
      "Validation file Info: FileObject(id='file-LT8NjQrKWvr2N9NWMyHMJZ', bytes=495784, created_at=1741724697, filename='mbpp_finetune_val.jsonl', object='file', purpose='fine-tune', status='processed', expires_at=None, status_details=None)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training file Info: {train_file}\")\n",
    "print(f\"Validation file Info: {valid_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5ca8081-8f78-442f-9493-b38dfafa7285",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = openai.fine_tuning.jobs.create(\n",
    "  training_file=train_file.id, \n",
    "  validation_file=valid_file.id,\n",
    "  model=\"gpt-4o-mini-2024-07-18\", \n",
    "  hyperparameters={\n",
    "    \"n_epochs\": 3,\n",
    "\t\"batch_size\": 3,\n",
    "\t\"learning_rate_multiplier\": 0.3\n",
    "  }\n",
    ")\n",
    "job_id = model.id\n",
    "status = model.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3a1e95be-b24a-491f-aad7-43ed3add4aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning model with jobID: ftjob-nAHqCOHrlROLsh4fzkDAvyb1.\n",
      "Training Response: FineTuningJob(id='ftjob-nAHqCOHrlROLsh4fzkDAvyb1', created_at=1741724879, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=3, learning_rate_multiplier=0.3, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-bgSJ5OvwR04OVBpr61Q2jq7K', result_files=[], seed=338309469, status='validating_files', trained_tokens=None, training_file='file-TLoZTPyqSHzprWM2vPEdLH', validation_file='file-LT8NjQrKWvr2N9NWMyHMJZ', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=3, learning_rate_multiplier=0.3, n_epochs=3)), type='supervised'), user_provided_suffix=None)\n",
      "Training Status: validating_files\n"
     ]
    }
   ],
   "source": [
    "print(f'Fine-tuning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {model}\")\n",
    "print(f\"Training Status: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1140943b-a447-465f-9142-265a7c65c32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-nAHqCOHrlROLsh4fzkDAvyb1', created_at=1741724879, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(batch_size=3, learning_rate_multiplier=0.3, n_epochs=3), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-bgSJ5OvwR04OVBpr61Q2jq7K', result_files=[], seed=338309469, status='validating_files', trained_tokens=None, training_file='file-TLoZTPyqSHzprWM2vPEdLH', validation_file='file-LT8NjQrKWvr2N9NWMyHMJZ', estimated_finish=None, integrations=[], metadata=None, method=Method(dpo=None, supervised=MethodSupervised(hyperparameters=MethodSupervisedHyperparameters(batch_size=3, learning_rate_multiplier=0.3, n_epochs=3)), type='supervised'), user_provided_suffix=None)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.fine_tuning.jobs.retrieve(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73363e2e-a6d9-45f4-9352-75177d6fce3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Job has already completed: ftjob-nAHqCOHrlROLsh4fzkDAvyb1', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'invalid_cancel'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m openai\u001b[38;5;241m.\u001b[39mfine_tuning\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mcancel(job_id)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\openai\\resources\\fine_tuning\\jobs\\jobs.py:293\u001b[0m, in \u001b[0;36mJobs.cancel\u001b[1;34m(self, fine_tuning_job_id, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fine_tuning_job_id:\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a non-empty value for `fine_tuning_job_id` but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuning_job_id\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/fine_tuning/jobs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfine_tuning_job_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/cancel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    295\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    296\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    297\u001b[0m     ),\n\u001b[0;32m    298\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mFineTuningJob,\n\u001b[0;32m    299\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1230\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1237\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1239\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1240\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1241\u001b[0m     )\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 919\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[0;32m    920\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m    921\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m    922\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[0;32m    923\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m    924\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m    925\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1020\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1022\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1026\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1027\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1031\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1032\u001b[0m )\n",
      "\u001b[1;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Job has already completed: ftjob-nAHqCOHrlROLsh4fzkDAvyb1', 'type': 'invalid_request_error', 'param': 'fine_tune_id', 'code': 'invalid_cancel'}}"
     ]
    }
   ],
   "source": [
    "openai.fine_tuning.jobs.cancel(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "00637999-313c-4818-b166-06b263be79be",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openai.fine_tuning.jobs.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9fe0821-ed48-4897-b99f-63f766260b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model = result.data[0].fine_tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4694ccf5-b9ae-42cc-920a-fb077cb41f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ft:gpt-4o-mini-2024-07-18:personal::BA15GnWM\n"
     ]
    }
   ],
   "source": [
    "print(fine_tuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b5999ddf-5d9d-44b6-a089-7c76482c8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide more context or specify what you want to print so I can assist you appropriately.\n"
     ]
    }
   ],
   "source": [
    "completion = openai.chat.completions.create(\n",
    "  model = fine_tuned_model,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are python expert and you provide answer only based on given context.\"},\n",
    "    {\"role\": \"user\", \"content\": \"generate code to print\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161d9c4-1ab8-4a2b-8d1e-c273070e0256",
   "metadata": {},
   "source": [
    "### Fine Tuning gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3aafafb6-ec04-4b1d-b2af-0f737bd282a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c48435fc-92de-473a-9ece-9507c35d1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA Available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "37090a94-bc56-485c-bd59-f42ac38de85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.float16,  \n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f879b948-8258-47c0-8138-3cf0435a2502",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m      2\u001b[0m     model_name,\n\u001b[0;32m      3\u001b[0m     quantization_config\u001b[38;5;241m=\u001b[39mbnb_config,\n\u001b[0;32m      4\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\transformers\\modeling_utils.py:262\u001b[0m, in \u001b[0;36mrestore_default_torch_dtype.<locals>._wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\transformers\\modeling_utils.py:4262\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4259\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m infer_auto_device_map(model, dtype\u001b[38;5;241m=\u001b[39mtarget_dtype, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdevice_map_kwargs)\n\u001b[0;32m   4261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4262\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mvalidate_environment(device_map\u001b[38;5;241m=\u001b[39mdevice_map)\n\u001b[0;32m   4264\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4265\u001b[0m     model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`from_pretrained`. Check \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.39.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "049f0cdb-372d-42d9-a22f-28e8cdf4b105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4be552f-facf-4f6f-9e98-53429e8fd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_model_copy = \"\"\"\n",
    "    ### User's questions:\n",
    "    {query}\n",
    "\n",
    "    ### Python version: \n",
    "    {python_version}\n",
    "    \n",
    "    ** Instructions **\n",
    "    - If user asks you to generate code and by using context you cannot do it, then generate it on your own\n",
    "    - If there is a code in your output explain this code to the user step by step\n",
    "    - Do not answer any other question than about python programming language\n",
    "    - If topic is complex provide summary at the end of your answer\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6d3d4c87-3569-44f5-b3f7-b3eacf3d6efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ### Context:\n",
      "    \n",
      "    \n",
      "    ### User's questions:\n",
      "    How can i open file in Python?\n",
      "\n",
      "    ### Python version: \n",
      "    3.10\n",
      "    \n",
      "    ** Instructions **\n",
      "    - If user asks you to generate code and by using context you cannot do it, then generate it on your own\n",
      "    - If user doesn't ask to generate code and the context does not contain answer for query say \"I don't have sufficient knowledge to answer this question\".\n",
      "    - If the user's question does not specify Python, rephrase it internally as a Python-related question before answering.\n",
      "    - If there is a code in your output explain this code to the user step by step\n",
      "    - Do not answer any other question than about python programming language\n",
      "    - If topic is complex provide summary at the end of your answer\n",
      "    - Do not make up any information\n",
      "    - Provide consise and structured answer\n",
      "    \n",
      "    ### Answer:\n",
      "    To open a file in Python, you can use the `open()` function. Here's an example:\n",
      "    \n",
      "    ```python\n",
      "    with open('filename.txt', 'r') as f:\n",
      "        contents = f.read()\n",
      "    ```\n",
      "    \n",
      "    In this example, we're opening a file named `filename.txt` with read mode (`r`) using the `with` statement. We're using the `with` statement because we want to close the file automatically when the `with` block ends. We're using the `open()` function to open the file in the `r` mode (read-only). We're then reading the contents of the file with the `read()` method. Finally, we're reading the contents of the file and storing it in a variable called `contents`. Note that you can also pass additional arguments to the `open()` function, like the file mode (`'w' for writing, 'a' for append, etc.) or the file encoding (`'utf-8' if the file is in UTF-8 format, for example).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_response(prompt, max_new_tokens=300):\n",
    "    prompt = prompt_model.format(context=\"\",query = prompt,python_version = \"3.10\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True, \n",
    "            temperature=0.7, \n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id  \n",
    "        )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(generate_response(\"How can i open file in Python?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5786b9e0-fb92-4386-a473-d7d5a362a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=2,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9f1e3731-39cd-4877-a199-adeb24b1d82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 281,600 || all params: 1,100,329,984 || trainable%: 0.0256\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fbd539d7-250e-4aec-b509-ac7d02a676b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in train_data[0][\"messages\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720215b1-b54b-450f-b643-d07d5c2754c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(sample_text)\n",
    "print(f\"Number of tokens: {len(tokenized['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f5395-a873-4bf7-9697-6ddec37cf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"json\", data_files=\"mbpp_finetune.jsonl\")[\"train\"]\n",
    "valid_data = load_dataset(\"json\", data_files=\"mbpp_finetune_val.jsonl\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "10572537-caf5-4180-a839-8e339fdce1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_data[0][\"messages\"][0][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ee80f400-6fd9-4c0a-ade8-da61ca87bf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfa42836c364a1cbf202093c1b03468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1496 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Column 1 named input_ids expected length 1000 but got length 1496",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m     full_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m messages])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(full_text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1496\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m tokenized_train \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m tokenized_valid \u001b[38;5;241m=\u001b[39m valid_data\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    560\u001b[0m }\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\datasets\\arrow_dataset.py:3079\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3074\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3075\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3076\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3077\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3078\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3079\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3080\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3081\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\datasets\\arrow_dataset.py:3534\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3532\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_table(batch\u001b[38;5;241m.\u001b[39mto_arrow())\n\u001b[0;32m   3533\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3534\u001b[0m         writer\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[0;32m   3535\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_examples_in_batch\n\u001b[0;32m   3536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\datasets\\arrow_writer.py:608\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    606\u001b[0m         inferred_features[col] \u001b[38;5;241m=\u001b[39m typed_sequence\u001b[38;5;241m.\u001b[39mget_inferred_type()\n\u001b[0;32m    607\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m--> 608\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_table(pa_table, writer_batch_size)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\pyarrow\\table.pxi:4868\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\pyarrow\\table.pxi:4214\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.validate\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pyhelper\\Lib\\site-packages\\pyarrow\\error.pxi:92\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowInvalid\u001b[0m: Column 1 named input_ids expected length 1000 but got length 1496"
     ]
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    messages = example[\"messages\"]\n",
    "\n",
    "    if isinstance(messages, list) and isinstance(messages[0], list):\n",
    "        messages = messages[0]\n",
    "\n",
    "    full_text = \"\\n\".join([f\"{msg['role'].capitalize()}: {msg['content']}\" for msg in messages])\n",
    "\n",
    "    return tokenizer(full_text, truncation=True, padding=\"max_length\", max_length=1496)\n",
    "\n",
    "tokenized_train = train_data.map(tokenize_function, batched=True)\n",
    "tokenized_valid = valid_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f3b67-5533-44ef-9959-a2b7dbfbd6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
